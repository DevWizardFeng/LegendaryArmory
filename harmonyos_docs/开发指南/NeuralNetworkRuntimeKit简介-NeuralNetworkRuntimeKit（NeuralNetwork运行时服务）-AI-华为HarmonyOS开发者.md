<h1 _ngcontent-nuu-c119="" class="doc-title ng-star-inserted" title="Neural Network Runtime Kit简介"> Neural Network Runtime Kit简介 </h1>

<div _ngcontent-nuu-c106="" auitextselectionexpansion="" class="markdown-body ng-star-inserted" style="position: relative;"> <div> <div class="tiledSection"><h2 id="使用场景">使用场景<i class="anchor-icon anchor-icon-link" anchorid="使用场景" tips="复制节点链接"></i></h2><p>Neural Network Runtime（NNRt, 神经网络运行时）是面向AI领域的跨芯片推理计算运行时，作为中间桥梁连通上层AI推理框架和底层加速芯片，实现AI模型的跨芯片推理计算。</p> <p>Neural Network Runtime的Native接口主要面向AI推理框架的开发者，或者希望直接使用AI加速硬件实现模型推理加速的应用开发者。</p> <p>AI推理框架可以调用NNRt的构图接口将推理框架的模型图转换为NNRt内部使用的模型图，然后调用NNRt的编译和执行接口在NNRt底层对接的AI加速硬件上进行模型推理。该方式可以实现无感知的跨AI硬件推理，但是首次加载模型速度较慢。</p> <p>AI推理框架和应用开发者也可以无需调用NNRt构图接口，直接使用某款具体硬件对应的离线模型在NNRt上执行模型推理。该方式仅能实现在特定AI硬件上执行推理，但是首次加载模型速度较快。</p> </div>  <div class="tiledSection"><h2 id="nnrt架构">NNRt架构<i class="anchor-icon anchor-icon-link" anchorid="nnrt架构" tips="复制节点链接"></i></h2><p>如图1所示，除了Native开放接口，NNRt软件架构包含如下几个功能模块：</p> <ol><li><strong>在线构图</strong>：AI推理框架需要调用NNRt的构图接口将推理框架的模型图转换为NNRt内部模型图。而系统内置的MindSpore Lite推理框架（具体可参考<a href="https://developer.huawei.com/consumer/cn/doc/harmonyos-guides/mindspore-lite-guidelines">MindSpore Lite Kit</a>）通过MindIR模型图对接NNRt。由于MindIR模型图和NNRt内部模型图格式兼容，因此MindSpore Lite无需调用NNRt的构图接口即可对接NNRt。</li><li><strong>模型编译</strong>：NNRt内部模型图或离线模型文件需要通过NNRt的编译接口在底层AI硬件驱动上编译为硬件相关的模型对象，后续就可以在该硬件上执行模型推理。</li><li><strong>模型推理</strong>：基于已编译的模型对象创建执行器，设置推理的输入和输出张量，然后在AI硬件上执行模型推理。</li><li><strong>内存管理</strong>：推理的输入和输出张量需要包含对应的数据内存，该模块负责在AI硬件驱动上申请共享内存并赋给张量，并在张量销毁时释放对应共享内存。通过AI硬件驱动上的共享内存可以实现输入和输出数据的“零拷贝”，提升推理性能。</li><li><strong>设备管理</strong>：负责展示NNRt对接的AI硬件信息，并提供了选择AI硬件的功能。</li><li><strong>模型缓存</strong>：已编译的模型对象写成模型缓存格式，保存在文件或一段内存中。在下一次编译模型时，可以直接从文件或内存形式的模型缓存中加载，大幅提升编译速度。</li><li><strong>离线模型推理</strong>：除了支持通过构图接口构造模型图，NNRt也支持直接使用AI硬件相关的模型文件（简称为离线模型）进行推理。应用开发者使用AI硬件厂商提供的模型转换器将原始训练模型转换为AI硬件对应的离线模型文件，并将它部署在应用程序中，在应用运行期间通过NNRt的离线模型编译接口传入。离线模型仅能在对应AI硬件上编译和推理，无法支持跨AI硬件兼容。但由于离线模型和硬件直接相关，因此编译速度通常很快。</li></ol> <p><strong>图1</strong> Neural Network Runtime架构图</p> <p><span><img originheight="2650" originwidth="3631" src="https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20251211114224.70730805402904123706980357511605:50001231000000:2800:CDA2D59B6C8794328D78E0A3800E0B438EC30369E661DCEBC3C41FA5677AE428.jpg" width="920" height="671.440374552465"></span></p> </div>  <div class="tiledSection"><h2 id="亮点特征">亮点特征<i class="anchor-icon anchor-icon-link" anchorid="亮点特征" tips="复制节点链接"></i></h2><ul><li>NNRt面向AI推理框架和AI应用开放了统一的AI加速硬件推理接口，可支持无感知的跨AI硬件推理。</li><li>NNRt提供了构图接口，可以让AI推理框架将内部模型图对接到NNRt。</li><li>NNRt提供了模型编译缓存功能，可将模型编译结果保存为缓存文件，大幅加快模型加载速度。</li><li>NNRt提供了硬件相关的离线模型加载功能，可缩短模型编译时间，但是仅可在对应AI硬件上执行。</li><li>NNRt提供了配置推理优先级、性能模式、FP16模式等常见硬件属性，也支持配置特定硬件的自定义扩展属性。</li><li>NNRt通过申请AI硬件驱动上的共享内存来实现数据的“零拷贝”，提升推理性能。</li></ul> </div>  <div class="tiledSection"><h2 id="能力范围">能力范围<i class="anchor-icon anchor-icon-link" anchorid="能力范围" tips="复制节点链接"></i></h2><ul><li>NNRt仅可提供已在底层接入的AI加速硬件的AI推理能力，不提供CPU等通用硬件上的AI推理能力。</li><li>NNRt仅能提供大多数AI硬件共有的基础AI推理能力和硬件属性配置，例如编译、执行、内存管理、优先级、性能模式等。如果希望配置某款AI硬件特有的硬件属性，可以通过NNRt提供的自定义扩展属性接口配置，具体属性名称和值需要查阅硬件厂商的文档。</li><li>NNRt目前支持常用算子56个，后续版本会逐步增加。注意NNRt的算子并没有具体实现，仅作为内部模型图的元素对接底层AI硬件，具体算子实现其实是在AI硬件驱动中。</li><li>NNRt目前仅支持同步推理，计划在后续版本支持异步推理。</li><li>NNRt不支持多线程并发构图，是否支持并发编译和执行取决于底层硬件驱动是否支持。</li></ul> </div>  <div class="tiledSection"><h2 id="与相关kit的关系">与相关Kit的关系<i class="anchor-icon anchor-icon-link" anchorid="与相关kit的关系" tips="复制节点链接"></i></h2><p>HarmonyOS AI开放层次由上层到底层分别为：</p> <ul><li><p><a href="https://developer.huawei.com/consumer/cn/doc/harmonyos-guides/mindspore-lite-kit-introduction" target="_blank">MindSpore Lite Kit</a>：HarmonyOS内置的轻量化AI引擎，提供统一推理接口和多后端硬件加速能力，使能手机、PC/2in1、TV等全场景智能应用。</p>  </li><li><p>Neural Network Runtime Kit：面向AI领域的跨芯片推理计算运行时，作为中间桥梁连通上层AI推理框架和底层加速芯片，实现AI模型的跨芯片推理计算。</p>  </li><li><p><a href="https://developer.huawei.com/consumer/cn/doc/harmonyos-guides/cannkit-introduction" target="_blank">CANN Kit</a>：海思AI硬件统一开放计算架构，支持 Ascend C NPU自定义编程、端云协同复用。擅长偏重载AI计算、需深度优化性能功耗场景。</p>  </li></ul> <p>Neural Network Runtime Kit可支持系统内置的MindSpore Lite推理框架（MindSpore Lite Kit），MindSpore Lite Kit作为已开放了配置NNRt的Native接口。</p> <p>MindSpore Lite Kit与NNRt接口对接时无需构图，两者共享同一份模型图格式（MindIR），因此使用MindSpore Lite Kit在NNRt上加载模型比其他AI推理框架更高效。此外，MindSpore Lite还支持通用硬件CPU/GPU与NNRt AI加速硬件之间的模型异构推理功能。</p> <p>CANN Kit作为麒麟平台的后端接入Neural Network Runtime Kit，支撑Neural Network Runtime Kit于麒麟平台的AI计算能力。CANN Kit构建在底层的硬件驱动和优化的计算库之上，与云侧昇腾芯片统一支持AscendC自定义算子编程语言和相关工具链，是HarmonyOS智能生态的重要基石，为众多领域的智能应用提供核心支撑。</p> </div>  <div class="tiledSection"><h2 id="模拟器支持情况">模拟器支持情况<i class="anchor-icon anchor-icon-link" anchorid="模拟器支持情况" tips="复制节点链接"></i></h2><p>本Kit不支持模拟器。</p> </div> </div> <div></div></div>