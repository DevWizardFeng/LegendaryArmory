<h1 _ngcontent-otr-c119="" class="doc-title ng-star-inserted" title="Neural Network Runtime对接AI推理框架开发指导"> Neural Network Runtime对接AI推理框架开发指导 </h1>

<div _ngcontent-otr-c106="" auitextselectionexpansion="" class="markdown-body ng-star-inserted" style="position: relative;">   <div>    <div class="tiledSection">     <h2 id="场景介绍">场景介绍<i class="anchor-icon anchor-icon-link" anchorid="场景介绍" tips="复制节点链接"></i></h2>          <p>Neural Network Runtime作为AI推理引擎和加速芯片的桥梁，为AI推理引擎提供精简的Native接口，满足推理引擎通过加速芯片执行端到端推理的需求。</p>     <p>本文以图1展示的Add单算子模型为例，介绍Neural Network Runtime的开发流程。Add算子包含两个输入、一个参数和一个输出，其中的activation参数用于指定Add算子中激活函数的类型。</p>     <p><strong>图1</strong> Add单算子网络示意图</p>     <p><span><img originheight="482" originwidth="1242" src="https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20251211114225.04401830507980637397707696632263:50001231000000:2800:FEA60DF299391496F8F3832EBB53B5249E03E6C91915FBFE53ADD0D56284E051.png" width="920" height="357.037037037037"></span></p>    </div>    <div class="tiledSection">     <h2 id="环境准备">环境准备<i class="anchor-icon anchor-icon-link" anchorid="环境准备" tips="复制节点链接"></i></h2>         </div>    <div class="tiledSection">     <h3 id="环境要求" class="firsth2">环境要求<i class="anchor-icon anchor-icon-link" anchorid="环境要求" tips="复制节点链接"></i></h3>          <p>Neural Network Runtime部件的环境要求如下：</p>     <ul>      <li>开发环境：Ubuntu 18.04及以上。</li>      <li>接入设备：系统定义的标准设备，系统中内置AI硬件驱动并已接入Neural Network Runtime。</li>     </ul>     <p>由于Neural Network Runtime通过Native API对外开放，需要下载对应的SDK并通过Native开发套件编译Neural Network Runtime应用。可以使用DevEco Studio来搭建环境和编译代码。</p>    </div>    <div class="tiledSection">     <h3 id="环境搭建">环境搭建<i class="anchor-icon anchor-icon-link" anchorid="环境搭建" tips="复制节点链接"></i></h3>          <ol>      <li>使用Ubuntu编译服务器的终端。</li>      <li>指定native工具链路径来编译代码，可以使用DevEco Studio来下载对应的SDK来进行编译。</li>      <li>DevEco Studio安装目录下的SDK路径可以在DevEco Studio工程界面，点击File &gt; Settings... &gt; 在settings中搜索SDK，下载对应的SDK即可。</li>     </ol>    </div>    <div class="tiledSection">     <h2 id="接口说明">接口说明<i class="anchor-icon anchor-icon-link" anchorid="接口说明" tips="复制节点链接"></i></h2>          <p>以下为Neural Network Runtime开发流程中的常用接口，具体可见<a href="https://developer.huawei.com/consumer/cn/doc/harmonyos-references/capi-neuralnetworkruntime" target="_blank">NeuralNetworkRuntime</a>。</p>    </div>    <div class="tiledSection">     <h3 id="结构体" class="firsth2">结构体<i class="anchor-icon anchor-icon-link" anchorid="结构体" tips="复制节点链接"></i></h3>          <div class="tablenoborder">      <div class="tbBox"><table class="layoutFixed idpTab">       <thead>        <tr>         <th align="left" class="cellrowborder" id="mcps1.3.6.2.1.3.1.1" valign="top" width="50%">结构体名称</th>         <th align="left" class="cellrowborder" id="mcps1.3.6.2.1.3.1.2" valign="top" width="50%">描述</th>        </tr>       </thead>               <tbody><tr>         <td class="cellrowborder" valign="top" width="50%">typedef struct OH_NNModel OH_NNModel</td>         <td class="cellrowborder" valign="top" width="50%">Neural Network Runtime的模型句柄，用于构造模型。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">typedef struct OH_NNCompilation OH_NNCompilation</td>         <td class="cellrowborder" valign="top" width="50%">Neural Network Runtime的编译器句柄，用于编译AI模型。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">typedef struct OH_NNExecutor OH_NNExecutor</td>         <td class="cellrowborder" valign="top" width="50%">Neural Network Runtime的执行器句柄，用于在指定设备上执行推理计算。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">typedef struct NN_QuantParam NN_QuantParam</td>         <td class="cellrowborder" valign="top" width="50%">Neural Network Runtime的量化参数句柄，用于在构造模型时指定张量的量化参数。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">typedef struct NN_TensorDesc NN_TensorDesc</td>         <td class="cellrowborder" valign="top" width="50%">Neural Network Runtime的张量描述句柄，用于描述张量的各类属性，例如数据布局、数据类型、形状等。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">typedef struct NN_Tensor NN_Tensor</td>         <td class="cellrowborder" valign="top" width="50%">Neural Network Runtime的张量句柄，用于设置执行器的推理输入和输出张量。</td>        </tr>             </tbody></table></div>     </div>    </div>    <div class="tiledSection">     <h3 id="模型构造接口">模型构造接口<i class="anchor-icon anchor-icon-link" anchorid="模型构造接口" tips="复制节点链接"></i></h3>          <div class="tablenoborder">      <div class="tbBox"><table class="layoutFixed idpTab">       <thead>        <tr>         <th align="left" class="cellrowborder" id="mcps1.3.7.2.1.3.1.1" valign="top" width="50%">接口名称</th>         <th align="left" class="cellrowborder" id="mcps1.3.7.2.1.3.1.2" valign="top" width="50%">描述</th>        </tr>       </thead>               <tbody><tr>         <td class="cellrowborder" valign="top" width="50%">OH_NNModel_Construct()</td>         <td class="cellrowborder" valign="top" width="50%">创建OH_NNModel类型的模型实例。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNModel_AddTensorToModel(OH_NNModel *model, const NN_TensorDesc *tensorDesc)</td>         <td class="cellrowborder" valign="top" width="50%">向模型实例中添加张量。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNModel_SetTensorData(OH_NNModel *model, uint32_t index, const void *dataBuffer, size_t length)</td>         <td class="cellrowborder" valign="top" width="50%">设置张量的数值。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNModel_AddOperation(OH_NNModel *model, OH_NN_OperationType op, const OH_NN_UInt32Array *paramIndices, const OH_NN_UInt32Array *inputIndices, const OH_NN_UInt32Array *outputIndices)</td>         <td class="cellrowborder" valign="top" width="50%">向模型实例中添加算子。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNModel_SpecifyInputsAndOutputs(OH_NNModel *model, const OH_NN_UInt32Array *inputIndices, const OH_NN_UInt32Array *outputIndices)</td>         <td class="cellrowborder" valign="top" width="50%">指定模型的输入和输出张量的索引值。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNModel_Finish(OH_NNModel *model)</td>         <td class="cellrowborder" valign="top" width="50%">完成模型构图。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">void OH_NNModel_Destroy(OH_NNModel **model)</td>         <td class="cellrowborder" valign="top" width="50%">销毁模型实例。</td>        </tr>             </tbody></table></div>     </div>    </div>    <div class="tiledSection">     <h3 id="模型编译接口">模型编译接口<i class="anchor-icon anchor-icon-link" anchorid="模型编译接口" tips="复制节点链接"></i></h3>          <div class="tablenoborder">      <div class="tbBox"><table class="layoutFixed idpTab">       <thead>        <tr>         <th align="left" class="cellrowborder" id="mcps1.3.8.2.1.3.1.1" valign="top" width="50%">接口名称</th>         <th align="left" class="cellrowborder" id="mcps1.3.8.2.1.3.1.2" valign="top" width="50%">描述</th>        </tr>       </thead>               <tbody><tr>         <td class="cellrowborder" valign="top" width="50%">OH_NNCompilation *OH_NNCompilation_Construct(const OH_NNModel *model)</td>         <td class="cellrowborder" valign="top" width="50%">基于模型实例创建OH_NNCompilation类型的编译实例。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NNCompilation *OH_NNCompilation_ConstructWithOfflineModelFile(const char *modelPath)</td>         <td class="cellrowborder" valign="top" width="50%">基于离线模型文件路径创建OH_NNCompilation类型的编译实例。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NNCompilation *OH_NNCompilation_ConstructWithOfflineModelBuffer(const void *modelBuffer, size_t modelSize)</td>         <td class="cellrowborder" valign="top" width="50%">基于离线模型文件内存创建OH_NNCompilation类型的编译实例。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NNCompilation *OH_NNCompilation_ConstructForCache()</td>         <td class="cellrowborder" valign="top" width="50%">创建一个空的编译实例，以便稍后从模型缓存中恢复。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNCompilation_ExportCacheToBuffer(OH_NNCompilation *compilation, const void *buffer, size_t length, size_t *modelSize)</td>         <td class="cellrowborder" valign="top" width="50%">将模型缓存写入到指定内存区域。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNCompilation_ImportCacheFromBuffer(OH_NNCompilation *compilation, const void *buffer, size_t modelSize)</td>         <td class="cellrowborder" valign="top" width="50%">从指定内存区域读取模型缓存。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNCompilation_AddExtensionConfig(OH_NNCompilation *compilation, const char *configName, const void *configValue, const size_t configValueSize)</td>         <td class="cellrowborder" valign="top" width="50%">为自定义硬件属性添加扩展配置，具体硬件的扩展属性名称和属性值需要从硬件厂商的文档中获取。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNCompilation_SetDevice(OH_NNCompilation *compilation, size_t deviceID)</td>         <td class="cellrowborder" valign="top" width="50%">指定模型编译和计算的硬件，可通过设备管理接口获取。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNCompilation_SetCache(OH_NNCompilation *compilation, const char *cachePath, uint32_t version)</td>         <td class="cellrowborder" valign="top" width="50%">设置编译模型的缓存目录和版本。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNCompilation_SetPerformanceMode(OH_NNCompilation *compilation, OH_NN_PerformanceMode performanceMode)</td>         <td class="cellrowborder" valign="top" width="50%">设置模型计算的性能模式。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNCompilation_SetPriority(OH_NNCompilation *compilation, OH_NN_Priority priority)</td>         <td class="cellrowborder" valign="top" width="50%">设置模型计算的优先级。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNCompilation_EnableFloat16(OH_NNCompilation *compilation, bool enableFloat16)</td>         <td class="cellrowborder" valign="top" width="50%">是否以float16的浮点数精度计算。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNCompilation_Build(OH_NNCompilation *compilation)</td>         <td class="cellrowborder" valign="top" width="50%">执行模型编译。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">void OH_NNCompilation_Destroy(OH_NNCompilation **compilation)</td>         <td class="cellrowborder" valign="top" width="50%">销毁编译实例。</td>        </tr>             </tbody></table></div>     </div>    </div>    <div class="tiledSection">     <h3 id="张量描述接口">张量描述接口<i class="anchor-icon anchor-icon-link" anchorid="张量描述接口" tips="复制节点链接"></i></h3>          <div class="tablenoborder">      <div class="tbBox"><table class="layoutFixed idpTab">       <thead>        <tr>         <th align="left" class="cellrowborder" id="mcps1.3.9.2.1.3.1.1" valign="top" width="50%">接口名称</th>         <th align="left" class="cellrowborder" id="mcps1.3.9.2.1.3.1.2" valign="top" width="50%">描述</th>        </tr>       </thead>               <tbody><tr>         <td class="cellrowborder" valign="top" width="50%">NN_TensorDesc *OH_NNTensorDesc_Create()</td>         <td class="cellrowborder" valign="top" width="50%">创建一个张量描述实例，用于后续创建张量。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNTensorDesc_SetName(NN_TensorDesc *tensorDesc, const char *name)</td>         <td class="cellrowborder" valign="top" width="50%">设置张量描述的名称。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNTensorDesc_GetName(const NN_TensorDesc *tensorDesc, const char **name)</td>         <td class="cellrowborder" valign="top" width="50%">获取张量描述的名称。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNTensorDesc_SetDataType(NN_TensorDesc *tensorDesc, OH_NN_DataType dataType)</td>         <td class="cellrowborder" valign="top" width="50%">设置张量描述的数据类型。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNTensorDesc_GetDataType(const NN_TensorDesc *tensorDesc, OH_NN_DataType *dataType)</td>         <td class="cellrowborder" valign="top" width="50%">获取张量描述的数据类型。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNTensorDesc_SetShape(NN_TensorDesc *tensorDesc, const int32_t *shape, size_t shapeLength)</td>         <td class="cellrowborder" valign="top" width="50%">设置张量描述的形状。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNTensorDesc_GetShape(const NN_TensorDesc *tensorDesc, int32_t **shape, size_t *shapeLength)</td>         <td class="cellrowborder" valign="top" width="50%">获取张量描述的形状。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNTensorDesc_SetFormat(NN_TensorDesc *tensorDesc, OH_NN_Format format)</td>         <td class="cellrowborder" valign="top" width="50%">设置张量描述的数据布局。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNTensorDesc_GetFormat(const NN_TensorDesc *tensorDesc, OH_NN_Format *format)</td>         <td class="cellrowborder" valign="top" width="50%">获取张量描述的数据布局。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNTensorDesc_GetElementCount(const NN_TensorDesc *tensorDesc, size_t *elementCount)</td>         <td class="cellrowborder" valign="top" width="50%">获取张量描述的元素个数。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNTensorDesc_GetByteSize(const NN_TensorDesc *tensorDesc, size_t *byteSize)</td>         <td class="cellrowborder" valign="top" width="50%">获取基于张量描述的形状和数据类型计算的数据占用字节数。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNTensorDesc_Destroy(NN_TensorDesc **tensorDesc)</td>         <td class="cellrowborder" valign="top" width="50%">销毁张量描述实例。</td>        </tr>             </tbody></table></div>     </div>    </div>    <div class="tiledSection">     <h3 id="张量接口">张量接口<i class="anchor-icon anchor-icon-link" anchorid="张量接口" tips="复制节点链接"></i></h3>          <div class="tablenoborder">      <div class="tbBox"><table class="layoutFixed idpTab">       <thead>        <tr>         <th align="left" class="cellrowborder" id="mcps1.3.10.2.1.3.1.1" valign="top" width="50%">接口名称</th>         <th align="left" class="cellrowborder" id="mcps1.3.10.2.1.3.1.2" valign="top" width="50%">描述</th>        </tr>       </thead>               <tbody><tr>         <td class="cellrowborder" valign="top" width="50%">NN_Tensor* OH_NNTensor_Create(size_t deviceID, NN_TensorDesc *tensorDesc)</td>         <td class="cellrowborder" valign="top" width="50%">从张量描述创建张量实例，会申请设备共享内存。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">NN_Tensor* OH_NNTensor_CreateWithSize(size_t deviceID, NN_TensorDesc *tensorDesc, size_t size)</td>         <td class="cellrowborder" valign="top" width="50%">按照指定内存大小和张量描述创建张量实例，会申请设备共享内存。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">NN_Tensor* OH_NNTensor_CreateWithFd(size_t deviceID, NN_TensorDesc *tensorDesc, int fd, size_t size, size_t offset)</td>         <td class="cellrowborder" valign="top" width="50%">按照指定共享内存的文件描述符和张量描述创建张量实例，从而可以复用其他张量的设备共享内存。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">NN_TensorDesc* OH_NNTensor_GetTensorDesc(const NN_Tensor *tensor)</td>         <td class="cellrowborder" valign="top" width="50%">获取张量内部的张量描述实例指针，从而可读取张量的属性，例如数据类型、形状等。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">void* OH_NNTensor_GetDataBuffer(const NN_Tensor *tensor)</td>         <td class="cellrowborder" valign="top" width="50%">获取张量数据的内存地址，可以读写张量数据。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNTensor_GetFd(const NN_Tensor *tensor, int *fd)</td>         <td class="cellrowborder" valign="top" width="50%">获取张量数据所在共享内存的文件描述符，文件描述符fd对应了一块设备共享内存。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNTensor_GetSize(const NN_Tensor *tensor, size_t *size)</td>         <td class="cellrowborder" valign="top" width="50%">获取张量数据所在共享内存的大小。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNTensor_GetOffset(const NN_Tensor *tensor, size_t *offset)</td>         <td class="cellrowborder" valign="top" width="50%">获取张量数据所在共享内存上的偏移量，张量数据可使用的大小为所在共享内存的大小减去偏移量。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNTensor_Destroy(NN_Tensor **tensor)</td>         <td class="cellrowborder" valign="top" width="50%">销毁张量实例。</td>        </tr>             </tbody></table></div>     </div>    </div>    <div class="tiledSection">     <h3 id="执行推理接口">执行推理接口<i class="anchor-icon anchor-icon-link" anchorid="执行推理接口" tips="复制节点链接"></i></h3>          <div class="tablenoborder">      <div class="tbBox"><table class="layoutFixed idpTab">       <thead>        <tr>         <th align="left" class="cellrowborder" id="mcps1.3.11.2.1.3.1.1" valign="top" width="50%">接口名称</th>         <th align="left" class="cellrowborder" id="mcps1.3.11.2.1.3.1.2" valign="top" width="50%">描述</th>        </tr>       </thead>               <tbody><tr>         <td class="cellrowborder" valign="top" width="50%">OH_NNExecutor *OH_NNExecutor_Construct(OH_NNCompilation *compilation)</td>         <td class="cellrowborder" valign="top" width="50%">创建OH_NNExecutor类型的执行器实例。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNExecutor_GetOutputShape(OH_NNExecutor *executor, uint32_t outputIndex, int32_t **shape, uint32_t *shapeLength)</td>         <td class="cellrowborder" valign="top" width="50%">获取输出张量的维度信息，用于输出张量具有动态形状的情况。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNExecutor_GetInputCount(const OH_NNExecutor *executor, size_t *inputCount)</td>         <td class="cellrowborder" valign="top" width="50%">获取输入张量的数量。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNExecutor_GetOutputCount(const OH_NNExecutor *executor, size_t *outputCount)</td>         <td class="cellrowborder" valign="top" width="50%">获取输出张量的数量。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">NN_TensorDesc* OH_NNExecutor_CreateInputTensorDesc(const OH_NNExecutor *executor, size_t index)</td>         <td class="cellrowborder" valign="top" width="50%">由指定索引值创建一个输入张量的描述，用于读取张量的属性或创建张量实例。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">NN_TensorDesc* OH_NNExecutor_CreateOutputTensorDesc(const OH_NNExecutor *executor, size_t index)</td>         <td class="cellrowborder" valign="top" width="50%">由指定索引值创建一个输出张量的描述，用于读取张量的属性或创建张量实例。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNExecutor_GetInputDimRange(const OH_NNExecutor *executor, size_t index, size_t **minInputDims, size_t **maxInputDims, size_t *shapeLength)</td>         <td class="cellrowborder" valign="top" width="50%">获取所有输入张量的维度范围。当输入张量具有动态形状时，不同设备可能支持不同的维度范围。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNExecutor_SetOnRunDone(OH_NNExecutor *executor, NN_OnRunDone onRunDone)</td>         <td class="cellrowborder" valign="top" width="50%">设置异步推理结束后的回调处理函数，回调函数定义详见接口文档。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNExecutor_SetOnServiceDied(OH_NNExecutor *executor, NN_OnServiceDied onServiceDied)</td>         <td class="cellrowborder" valign="top" width="50%">设置异步推理执行期间设备驱动服务突然死亡时的回调处理函数，回调函数定义详见接口文档。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNExecutor_RunSync(OH_NNExecutor *executor, NN_Tensor *inputTensor[], size_t inputCount, NN_Tensor *outputTensor[], size_t outputCount)</td>         <td class="cellrowborder" valign="top" width="50%">执行同步推理。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNExecutor_RunAsync(OH_NNExecutor *executor, NN_Tensor *inputTensor[], size_t inputCount, NN_Tensor *outputTensor[], size_t outputCount, int32_t timeout, void *userData)</td>         <td class="cellrowborder" valign="top" width="50%">执行异步推理。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">void OH_NNExecutor_Destroy(OH_NNExecutor **executor)</td>         <td class="cellrowborder" valign="top" width="50%">销毁执行器实例。</td>        </tr>             </tbody></table></div>     </div>    </div>    <div class="tiledSection">     <h3 id="设备管理接口">设备管理接口<i class="anchor-icon anchor-icon-link" anchorid="设备管理接口" tips="复制节点链接"></i></h3>          <div class="tablenoborder">      <div class="tbBox"><table class="layoutFixed idpTab">       <thead>        <tr>         <th align="left" class="cellrowborder" id="mcps1.3.12.2.1.3.1.1" valign="top" width="50%">接口名称</th>         <th align="left" class="cellrowborder" id="mcps1.3.12.2.1.3.1.2" valign="top" width="50%">描述</th>        </tr>       </thead>               <tbody><tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNDevice_GetAllDevicesID(const size_t **allDevicesID, uint32_t *deviceCount)</td>         <td class="cellrowborder" valign="top" width="50%">获取对接到Neural Network Runtime的所有硬件ID。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNDevice_GetName(size_t deviceID, const char **name)</td>         <td class="cellrowborder" valign="top" width="50%">获取指定硬件的名称。</td>        </tr>        <tr>         <td class="cellrowborder" valign="top" width="50%">OH_NN_ReturnCode OH_NNDevice_GetType(size_t deviceID, OH_NN_DeviceType *deviceType)</td>         <td class="cellrowborder" valign="top" width="50%">获取指定硬件的类别信息。</td>        </tr>             </tbody></table></div>     </div>    </div>    <div class="tiledSection">     <h2 id="开发步骤">开发步骤<i class="anchor-icon anchor-icon-link" anchorid="开发步骤" tips="复制节点链接"></i></h2>          <p>Neural Network Runtime的开发流程主要包含<strong>模型构造</strong>、<strong>模型编译</strong>和<strong>推理执行</strong>三个阶段。以下开发步骤以Add单算子模型为例，介绍调用Neural Network Runtime接口，开发应用的过程。</p>     <ol>      <li>       <p>创建应用样例文件。</p>       <p>首先，创建Neural Network Runtime应用样例的源文件。在项目目录下执行以下命令，创建nnrt_example/目录，并在目录下创建 nnrt_example.cpp 源文件。</p>       <div _ngcontent-otr-c106="" class="highlight-div"><div _ngcontent-otr-c106="" class="highlight-div-header"><div _ngcontent-otr-c106="" class="highlight-div-header-left"><div _ngcontent-otr-c106="" class="handle-button expand-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">收起</div></div></div><div _ngcontent-otr-c106="" class="highlight-div-header-right"><div _ngcontent-otr-c106="" class="handle-button ai-button"></div><div _ngcontent-otr-c106="" class="handle-button line-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">自动换行</div></div><div _ngcontent-otr-c106="" class="handle-button theme-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">深色代码主题</div></div><div _ngcontent-otr-c106="" class="handle-button copy-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">复制</div></div></div></div><div _ngcontent-otr-c106="" class="highlight-scroll-div"><pre class="shell prettyprint linenums hljs language-shell" hw-language="shell" data-highlighted="yes"><ol class="linenums"><li>mkdir ~/nnrt_example &amp;&amp; cd ~/nnrt_example</li><li>touch nnrt_example.cpp</li></ol></pre></div></div></li>      <li>       <p>导入Neural Network Runtime。</p>       <p>在 nnrt_example.cpp 文件的开头添加以下代码，引入Neural Network Runtime。</p>       <div _ngcontent-otr-c106="" class="highlight-div"><div _ngcontent-otr-c106="" class="highlight-div-header"><div _ngcontent-otr-c106="" class="highlight-div-header-left"><div _ngcontent-otr-c106="" class="handle-button expand-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">收起</div></div></div><div _ngcontent-otr-c106="" class="highlight-div-header-right"><div _ngcontent-otr-c106="" class="handle-button ai-button"></div><div _ngcontent-otr-c106="" class="handle-button line-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">自动换行</div></div><div _ngcontent-otr-c106="" class="handle-button theme-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">深色代码主题</div></div><div _ngcontent-otr-c106="" class="handle-button copy-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">复制</div></div></div></div><div _ngcontent-otr-c106="" class="highlight-scroll-div"><pre class="cpp prettyprint linenums hljs language-cpp" hw-language="cpp" data-highlighted="yes"><ol class="linenums"><li><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span></li><li><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstdarg&gt;</span></span></li><li><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">"neural_network_runtime/neural_network_runtime.h"</span></span></li></ol></pre></div></div></li>      <li>       <p>定义日志打印、设置输入数据、数据打印等辅助函数。</p>       <div _ngcontent-otr-c106="" class="highlight-div"><div _ngcontent-otr-c106="" class="highlight-div-header"><div _ngcontent-otr-c106="" class="highlight-div-header-left"><div _ngcontent-otr-c106="" class="handle-button expand-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">收起</div></div></div><div _ngcontent-otr-c106="" class="highlight-div-header-right"><div _ngcontent-otr-c106="" class="handle-button ai-button"></div><div _ngcontent-otr-c106="" class="handle-button line-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">自动换行</div></div><div _ngcontent-otr-c106="" class="handle-button theme-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">深色代码主题</div></div><div _ngcontent-otr-c106="" class="handle-button copy-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">复制</div></div></div></div><div _ngcontent-otr-c106="" class="highlight-scroll-div"><pre class="cpp prettyprint linenums hljs language-cpp" hw-language="cpp" data-highlighted="yes"><ol class="linenums"><li><span class="hljs-comment">// 返回值检查宏</span></li><li><span class="hljs-meta">#<span class="hljs-keyword">define</span> CHECKNEQ(realRet, expectRet, retValue, ...) \</span></li><li><span class="hljs-meta">    do { \</span></li><li><span class="hljs-meta">        <span class="hljs-keyword">if</span> ((realRet) != (expectRet)) { \</span></li><li><span class="hljs-meta">            printf(__VA_ARGS__); \</span></li><li><span class="hljs-meta">            return (retValue); \</span></li><li><span class="hljs-meta">        } \</span></li><li><span class="hljs-meta">    } while (0)</span></li><li>
</li><li><span class="hljs-meta">#<span class="hljs-keyword">define</span> CHECKEQ(realRet, expectRet, retValue, ...) \</span></li><li><span class="hljs-meta">    do { \</span></li><li><span class="hljs-meta">        <span class="hljs-keyword">if</span> ((realRet) == (expectRet)) { \</span></li><li><span class="hljs-meta">            printf(__VA_ARGS__); \</span></li><li><span class="hljs-meta">            return (retValue); \</span></li><li><span class="hljs-meta">        } \</span></li><li><span class="hljs-meta">    } while (0)</span></li><li>
</li><li><span class="hljs-comment">// 设置输入数据用于推理</span></li><li><span class="hljs-function">OH_NN_ReturnCode <span class="hljs-title">SetInputData</span><span class="hljs-params">(NN_Tensor* inputTensor[], <span class="hljs-type">size_t</span> inputSize)</span></span></li><li><span class="hljs-function"></span>{</li><li>    <span class="hljs-function">OH_NN_DataType <span class="hljs-title">dataType</span><span class="hljs-params">(OH_NN_FLOAT32)</span></span>;</li><li>    OH_NN_ReturnCode ret{OH_NN_FAILED};</li><li>    <span class="hljs-type">size_t</span> elementCount = <span class="hljs-number">0</span>;</li><li>    <span class="hljs-keyword">for</span> (<span class="hljs-type">size_t</span> i = <span class="hljs-number">0</span>; i &lt; inputSize; ++i) {</li><li>        <span class="hljs-comment">// 获取张量的数据内存</span></li><li>        <span class="hljs-keyword">auto</span> data = <span class="hljs-built_in">OH_NNTensor_GetDataBuffer</span>(inputTensor[i]);</li><li>        <span class="hljs-built_in">CHECKEQ</span>(data, <span class="hljs-literal">nullptr</span>, OH_NN_FAILED, <span class="hljs-string">"Failed to get data buffer."</span>);</li><li>        <span class="hljs-comment">// 获取张量的描述</span></li><li>        <span class="hljs-keyword">auto</span> desc = <span class="hljs-built_in">OH_NNTensor_GetTensorDesc</span>(inputTensor[i]);</li><li>        <span class="hljs-built_in">CHECKEQ</span>(desc, <span class="hljs-literal">nullptr</span>, OH_NN_FAILED, <span class="hljs-string">"Failed to get desc."</span>);</li><li>        <span class="hljs-comment">// 获取张量的数据类型</span></li><li>        ret = <span class="hljs-built_in">OH_NNTensorDesc_GetDataType</span>(desc, &amp;dataType);</li><li>        <span class="hljs-built_in">CHECKNEQ</span>(ret, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Failed to get data type."</span>);</li><li>        <span class="hljs-comment">// 获取张量的元素个数</span></li><li>        ret = <span class="hljs-built_in">OH_NNTensorDesc_GetElementCount</span>(desc, &amp;elementCount);</li><li>        <span class="hljs-built_in">CHECKNEQ</span>(ret, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Failed to get element count."</span>);</li><li>        <span class="hljs-keyword">switch</span>(dataType) {</li><li>            <span class="hljs-keyword">case</span> OH_NN_FLOAT32: {</li><li>                <span class="hljs-type">float</span>* floatValue = <span class="hljs-built_in">reinterpret_cast</span>&lt;<span class="hljs-type">float</span>*&gt;(data);</li><li>                <span class="hljs-keyword">for</span> (<span class="hljs-type">size_t</span> j = <span class="hljs-number">0</span>; j &lt; elementCount; ++j) {</li><li>                    floatValue[j] = <span class="hljs-built_in">static_cast</span>&lt;<span class="hljs-type">float</span>&gt;(j);</li><li>                }</li><li>                <span class="hljs-keyword">break</span>;</li><li>            }</li><li>            <span class="hljs-keyword">case</span> OH_NN_INT32: {</li><li>                <span class="hljs-type">int</span>* intValue = <span class="hljs-built_in">reinterpret_cast</span>&lt;<span class="hljs-type">int</span>*&gt;(data);</li><li>                <span class="hljs-keyword">for</span> (<span class="hljs-type">size_t</span> j = <span class="hljs-number">0</span>; j &lt; elementCount; ++j) {</li><li>                    intValue[j] = <span class="hljs-built_in">static_cast</span>&lt;<span class="hljs-type">int</span>&gt;(j);</li><li>                }</li><li>                <span class="hljs-keyword">break</span>;</li><li>            }</li><li>            <span class="hljs-keyword">default</span>:</li><li>                <span class="hljs-keyword">return</span> OH_NN_FAILED;</li><li>        }</li><li>    }</li><li>    <span class="hljs-keyword">return</span> OH_NN_SUCCESS;</li><li>}</li><li>
</li><li><span class="hljs-function">OH_NN_ReturnCode <span class="hljs-title">Print</span><span class="hljs-params">(NN_Tensor* outputTensor[], <span class="hljs-type">size_t</span> outputSize)</span></span></li><li><span class="hljs-function"></span>{</li><li>    <span class="hljs-function">OH_NN_DataType <span class="hljs-title">dataType</span><span class="hljs-params">(OH_NN_FLOAT32)</span></span>;</li><li>    OH_NN_ReturnCode ret{OH_NN_FAILED};</li><li>    <span class="hljs-type">size_t</span> elementCount = <span class="hljs-number">0</span>;</li><li>    <span class="hljs-keyword">for</span> (<span class="hljs-type">size_t</span> i = <span class="hljs-number">0</span>; i &lt; outputSize; ++i) {</li><li>        <span class="hljs-keyword">auto</span> data = <span class="hljs-built_in">OH_NNTensor_GetDataBuffer</span>(outputTensor[i]);</li><li>        <span class="hljs-built_in">CHECKEQ</span>(data, <span class="hljs-literal">nullptr</span>, OH_NN_FAILED, <span class="hljs-string">"Failed to get data buffer."</span>);</li><li>        <span class="hljs-keyword">auto</span> desc = <span class="hljs-built_in">OH_NNTensor_GetTensorDesc</span>(outputTensor[i]);</li><li>        <span class="hljs-built_in">CHECKEQ</span>(desc, <span class="hljs-literal">nullptr</span>, OH_NN_FAILED, <span class="hljs-string">"Failed to get desc."</span>);</li><li>        ret = <span class="hljs-built_in">OH_NNTensorDesc_GetDataType</span>(desc, &amp;dataType);</li><li>        <span class="hljs-built_in">CHECKNEQ</span>(ret, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Failed to get data type."</span>);</li><li>        ret = <span class="hljs-built_in">OH_NNTensorDesc_GetElementCount</span>(desc, &amp;elementCount);</li><li>        <span class="hljs-built_in">CHECKNEQ</span>(ret, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Failed to get element count."</span>);</li><li>        <span class="hljs-keyword">switch</span>(dataType) {</li><li>            <span class="hljs-keyword">case</span> OH_NN_FLOAT32: {</li><li>                <span class="hljs-type">float</span>* floatValue = <span class="hljs-built_in">reinterpret_cast</span>&lt;<span class="hljs-type">float</span>*&gt;(data);</li><li>                <span class="hljs-keyword">for</span> (<span class="hljs-type">size_t</span> j = <span class="hljs-number">0</span>; j &lt; elementCount; ++j) {</li><li>                    std::cout &lt;&lt; <span class="hljs-string">"Output index: "</span> &lt;&lt; j &lt;&lt; <span class="hljs-string">", value is: "</span> &lt;&lt; floatValue[j] &lt;&lt; <span class="hljs-string">"."</span> &lt;&lt; std::endl;</li><li>                }</li><li>                <span class="hljs-keyword">break</span>;</li><li>            }</li><li>            <span class="hljs-keyword">case</span> OH_NN_INT32: {</li><li>                <span class="hljs-type">int</span>* intValue = <span class="hljs-built_in">reinterpret_cast</span>&lt;<span class="hljs-type">int</span>*&gt;(data);</li><li>                <span class="hljs-keyword">for</span> (<span class="hljs-type">size_t</span> j = <span class="hljs-number">0</span>; j &lt; elementCount; ++j) {</li><li>                    std::cout &lt;&lt; <span class="hljs-string">"Output index: "</span> &lt;&lt; j &lt;&lt; <span class="hljs-string">", value is: "</span> &lt;&lt; intValue[j] &lt;&lt; <span class="hljs-string">"."</span> &lt;&lt; std::endl;</li><li>                }</li><li>                <span class="hljs-keyword">break</span>;</li><li>            }</li><li>            <span class="hljs-keyword">default</span>:</li><li>                <span class="hljs-keyword">return</span> OH_NN_FAILED;</li><li>        }</li><li>    }</li><li>
</li><li>    <span class="hljs-keyword">return</span> OH_NN_SUCCESS;</li><li>}</li></ol></pre></div></div></li>      <li>       <p>构造模型。</p>       <p>使用Neural Network Runtime的模型构造接口，构造Add单算子样例模型。</p>       <div _ngcontent-otr-c106="" class="highlight-div"><div _ngcontent-otr-c106="" class="highlight-div-header"><div _ngcontent-otr-c106="" class="highlight-div-header-left"><div _ngcontent-otr-c106="" class="handle-button expand-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">收起</div></div></div><div _ngcontent-otr-c106="" class="highlight-div-header-right"><div _ngcontent-otr-c106="" class="handle-button ai-button"></div><div _ngcontent-otr-c106="" class="handle-button line-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">自动换行</div></div><div _ngcontent-otr-c106="" class="handle-button theme-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">深色代码主题</div></div><div _ngcontent-otr-c106="" class="handle-button copy-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">复制</div></div></div></div><div _ngcontent-otr-c106="" class="highlight-scroll-div"><pre class="cpp prettyprint linenums hljs language-cpp" hw-language="cpp" data-highlighted="yes"><ol class="linenums"><li><span class="hljs-function">OH_NN_ReturnCode <span class="hljs-title">BuildModel</span><span class="hljs-params">(OH_NNModel** pmodel)</span></span></li><li><span class="hljs-function"></span>{</li><li>    <span class="hljs-comment">// 创建模型实例model，进行模型构造</span></li><li>    OH_NNModel* model = <span class="hljs-built_in">OH_NNModel_Construct</span>();</li><li>    <span class="hljs-built_in">CHECKEQ</span>(model, <span class="hljs-literal">nullptr</span>, OH_NN_FAILED, <span class="hljs-string">"Create model failed."</span>);</li><li>
</li><li>    <span class="hljs-comment">// 添加Add算子的第一个输入张量，类型为float32，张量形状为[1, 2, 2, 3]</span></li><li>    NN_TensorDesc* tensorDesc = <span class="hljs-built_in">OH_NNTensorDesc_Create</span>();</li><li>    <span class="hljs-built_in">CHECKEQ</span>(tensorDesc, <span class="hljs-literal">nullptr</span>, OH_NN_FAILED, <span class="hljs-string">"Create TensorDesc failed."</span>);</li><li>
</li><li>    <span class="hljs-type">int32_t</span> inputDims[<span class="hljs-number">4</span>] = {<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>};</li><li>    <span class="hljs-keyword">auto</span> returnCode = <span class="hljs-built_in">OH_NNTensorDesc_SetShape</span>(tensorDesc, inputDims, <span class="hljs-number">4</span>);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Set TensorDesc shape failed."</span>);</li><li>
</li><li>    returnCode = <span class="hljs-built_in">OH_NNTensorDesc_SetDataType</span>(tensorDesc, OH_NN_FLOAT32);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Set TensorDesc data type failed."</span>);</li><li>
</li><li>    returnCode = <span class="hljs-built_in">OH_NNTensorDesc_SetFormat</span>(tensorDesc, OH_NN_FORMAT_NONE);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Set TensorDesc format failed."</span>);</li><li>
</li><li>    returnCode = <span class="hljs-built_in">OH_NNModel_AddTensorToModel</span>(model, tensorDesc);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Add first TensorDesc to model failed."</span>);</li><li>
</li><li>    returnCode = <span class="hljs-built_in">OH_NNModel_SetTensorType</span>(model, <span class="hljs-number">0</span>, OH_NN_TENSOR);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Set model tensor type failed."</span>);</li><li>
</li><li>    <span class="hljs-comment">// 添加Add算子的第二个输入张量，类型为float32，张量形状为[1, 2, 2, 3]</span></li><li>    tensorDesc = <span class="hljs-built_in">OH_NNTensorDesc_Create</span>();</li><li>    <span class="hljs-built_in">CHECKEQ</span>(tensorDesc, <span class="hljs-literal">nullptr</span>, OH_NN_FAILED, <span class="hljs-string">"Create TensorDesc failed."</span>);</li><li>
</li><li>    returnCode = <span class="hljs-built_in">OH_NNTensorDesc_SetShape</span>(tensorDesc, inputDims, <span class="hljs-number">4</span>);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Set TensorDesc shape failed."</span>);</li><li>
</li><li>    returnCode = <span class="hljs-built_in">OH_NNTensorDesc_SetDataType</span>(tensorDesc, OH_NN_FLOAT32);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Set TensorDesc data type failed."</span>);</li><li>
</li><li>    returnCode = <span class="hljs-built_in">OH_NNTensorDesc_SetFormat</span>(tensorDesc, OH_NN_FORMAT_NONE);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Set TensorDesc format failed."</span>);</li><li>
</li><li>    returnCode = <span class="hljs-built_in">OH_NNModel_AddTensorToModel</span>(model, tensorDesc);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Add second TensorDesc to model failed."</span>);</li><li>
</li><li>    returnCode = <span class="hljs-built_in">OH_NNModel_SetTensorType</span>(model, <span class="hljs-number">1</span>, OH_NN_TENSOR);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Set model tensor type failed."</span>);</li><li>
</li><li>    <span class="hljs-comment">// 添加Add算子的参数张量，该参数张量用于指定激活函数的类型，张量的数据类型为int8。</span></li><li>    tensorDesc = <span class="hljs-built_in">OH_NNTensorDesc_Create</span>();</li><li>    <span class="hljs-built_in">CHECKEQ</span>(tensorDesc, <span class="hljs-literal">nullptr</span>, OH_NN_FAILED, <span class="hljs-string">"Create TensorDesc failed."</span>);</li><li>
</li><li>    <span class="hljs-type">int32_t</span> activationDims = <span class="hljs-number">1</span>;</li><li>    returnCode = <span class="hljs-built_in">OH_NNTensorDesc_SetShape</span>(tensorDesc, &amp;activationDims, <span class="hljs-number">1</span>);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Set TensorDesc shape failed."</span>);</li><li>
</li><li>    returnCode = <span class="hljs-built_in">OH_NNTensorDesc_SetDataType</span>(tensorDesc, OH_NN_INT8);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Set TensorDesc data type failed."</span>);</li><li>
</li><li>    returnCode = <span class="hljs-built_in">OH_NNTensorDesc_SetFormat</span>(tensorDesc, OH_NN_FORMAT_NONE);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Set TensorDesc format failed."</span>);</li><li>
</li><li>    returnCode = <span class="hljs-built_in">OH_NNModel_AddTensorToModel</span>(model, tensorDesc);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Add second TensorDesc to model failed."</span>);</li><li>
</li><li>    returnCode = <span class="hljs-built_in">OH_NNModel_SetTensorType</span>(model, <span class="hljs-number">2</span>, OH_NN_ADD_ACTIVATIONTYPE);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Set model tensor type failed."</span>);</li><li>
</li><li>    <span class="hljs-comment">// 将激活函数类型设置为OH_NN_FUSED_NONE，表示该算子不添加激活函数。</span></li><li>    <span class="hljs-type">int8_t</span> activationValue = OH_NN_FUSED_NONE;</li><li>    returnCode = <span class="hljs-built_in">OH_NNModel_SetTensorData</span>(model, <span class="hljs-number">2</span>, &amp;activationValue, <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">int8_t</span>));</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Set model tensor data failed."</span>);</li><li>
</li><li>    <span class="hljs-comment">// 设置Add算子的输出张量，类型为float32，张量形状为[1, 2, 2, 3]</span></li><li>    tensorDesc = <span class="hljs-built_in">OH_NNTensorDesc_Create</span>();</li><li>    <span class="hljs-built_in">CHECKEQ</span>(tensorDesc, <span class="hljs-literal">nullptr</span>, OH_NN_FAILED, <span class="hljs-string">"Create TensorDesc failed."</span>);</li><li>
</li><li>    returnCode = <span class="hljs-built_in">OH_NNTensorDesc_SetShape</span>(tensorDesc, inputDims, <span class="hljs-number">4</span>);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Set TensorDesc shape failed."</span>);</li><li>
</li><li>    returnCode = <span class="hljs-built_in">OH_NNTensorDesc_SetDataType</span>(tensorDesc, OH_NN_FLOAT32);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Set TensorDesc data type failed."</span>);</li><li>
</li><li>    returnCode = <span class="hljs-built_in">OH_NNTensorDesc_SetFormat</span>(tensorDesc, OH_NN_FORMAT_NONE);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Set TensorDesc format failed."</span>);</li><li>
</li><li>    returnCode = <span class="hljs-built_in">OH_NNModel_AddTensorToModel</span>(model, tensorDesc);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Add forth TensorDesc to model failed."</span>);</li><li>
</li><li>    returnCode = <span class="hljs-built_in">OH_NNModel_SetTensorType</span>(model, <span class="hljs-number">3</span>, OH_NN_TENSOR);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Set model tensor type failed."</span>);</li><li>
</li><li>    <span class="hljs-comment">// 指定Add算子的输入张量、参数张量和输出张量的索引</span></li><li>    <span class="hljs-type">uint32_t</span> inputIndicesValues[<span class="hljs-number">2</span>] = {<span class="hljs-number">0</span>, <span class="hljs-number">1</span>};</li><li>    <span class="hljs-type">uint32_t</span> paramIndicesValues = <span class="hljs-number">2</span>;</li><li>    <span class="hljs-type">uint32_t</span> outputIndicesValues = <span class="hljs-number">3</span>;</li><li>    OH_NN_UInt32Array paramIndices = {&amp;paramIndicesValues, <span class="hljs-number">1</span>};</li><li>    OH_NN_UInt32Array inputIndices = {inputIndicesValues, <span class="hljs-number">2</span>};</li><li>    OH_NN_UInt32Array outputIndices = {&amp;outputIndicesValues, <span class="hljs-number">1</span>};</li><li>
</li><li>    <span class="hljs-comment">// 向模型实例添加Add算子</span></li><li>    returnCode = <span class="hljs-built_in">OH_NNModel_AddOperation</span>(model, OH_NN_OPS_ADD, &amp;paramIndices, &amp;inputIndices, &amp;outputIndices);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Add operation to model failed."</span>);</li><li>
</li><li>    <span class="hljs-comment">// 设置模型实例的输入张量、输出张量的索引</span></li><li>    returnCode = <span class="hljs-built_in">OH_NNModel_SpecifyInputsAndOutputs</span>(model, &amp;inputIndices, &amp;outputIndices);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Specify model inputs and outputs failed."</span>);</li><li>
</li><li>    <span class="hljs-comment">// 完成模型实例的构建</span></li><li>    returnCode = <span class="hljs-built_in">OH_NNModel_Finish</span>(model);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"Build model failed."</span>);</li><li>
</li><li>    <span class="hljs-comment">// 返回模型实例</span></li><li>    *pmodel = model;</li><li>    <span class="hljs-keyword">return</span> OH_NN_SUCCESS;</li><li>}</li></ol></pre></div></div></li>      <li>       <p>查询Neural Network Runtime已经对接的AI加速芯片。</p>       <p>Neural Network Runtime支持通过HDI接口，对接多种AI加速芯片。在执行模型编译前，需要查询当前设备下，Neural Network Runtime已经对接的AI加速芯片。每个AI加速芯片对应唯一的ID值，在编译阶段需要通过设备ID，指定模型编译的芯片。</p>       <div _ngcontent-otr-c106="" class="highlight-div"><div _ngcontent-otr-c106="" class="highlight-div-header"><div _ngcontent-otr-c106="" class="highlight-div-header-left"><div _ngcontent-otr-c106="" class="handle-button expand-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">收起</div></div></div><div _ngcontent-otr-c106="" class="highlight-div-header-right"><div _ngcontent-otr-c106="" class="handle-button ai-button"></div><div _ngcontent-otr-c106="" class="handle-button line-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">自动换行</div></div><div _ngcontent-otr-c106="" class="handle-button theme-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">深色代码主题</div></div><div _ngcontent-otr-c106="" class="handle-button copy-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">复制</div></div></div></div><div _ngcontent-otr-c106="" class="highlight-scroll-div"><pre class="cpp prettyprint linenums hljs language-cpp" hw-language="cpp" data-highlighted="yes"><ol class="linenums"><li><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">GetAvailableDevices</span><span class="hljs-params">(std::vector&lt;<span class="hljs-type">size_t</span>&gt;&amp; availableDevice)</span></span></li><li><span class="hljs-function"></span>{</li><li>    availableDevice.<span class="hljs-built_in">clear</span>();</li><li>
</li><li>    <span class="hljs-comment">// 获取可用的硬件ID</span></li><li>    <span class="hljs-type">const</span> <span class="hljs-type">size_t</span>* devices = <span class="hljs-literal">nullptr</span>;</li><li>    <span class="hljs-type">uint32_t</span> deviceCount = <span class="hljs-number">0</span>;</li><li>    OH_NN_ReturnCode ret = <span class="hljs-built_in">OH_NNDevice_GetAllDevicesID</span>(&amp;devices, &amp;deviceCount);</li><li>    <span class="hljs-keyword">if</span> (ret != OH_NN_SUCCESS) {</li><li>        std::cout &lt;&lt; <span class="hljs-string">"GetAllDevicesID failed, get no available device."</span> &lt;&lt; std::endl;</li><li>        <span class="hljs-keyword">return</span>;</li><li>    }</li><li>
</li><li>    <span class="hljs-keyword">for</span> (<span class="hljs-type">uint32_t</span> i = <span class="hljs-number">0</span>; i &lt; deviceCount; i++) {</li><li>        availableDevice.<span class="hljs-built_in">emplace_back</span>(devices[i]);</li><li>    }</li><li>}</li></ol></pre></div></div></li>      <li>       <p>在指定的设备上编译模型。</p>       <p>Neural Network Runtime使用抽象的模型表达描述AI模型的拓扑结构。在AI加速芯片上执行前，需要通过Neural Network Runtime提供的编译模块来创建编译实例，并由编译实例将抽象的模型表达下发至芯片驱动层，转换成可以直接推理计算的格式，即模型编译。</p>       <div _ngcontent-otr-c106="" class="highlight-div"><div _ngcontent-otr-c106="" class="highlight-div-header"><div _ngcontent-otr-c106="" class="highlight-div-header-left"><div _ngcontent-otr-c106="" class="handle-button expand-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">收起</div></div></div><div _ngcontent-otr-c106="" class="highlight-div-header-right"><div _ngcontent-otr-c106="" class="handle-button ai-button"></div><div _ngcontent-otr-c106="" class="handle-button line-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">自动换行</div></div><div _ngcontent-otr-c106="" class="handle-button theme-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">深色代码主题</div></div><div _ngcontent-otr-c106="" class="handle-button copy-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">复制</div></div></div></div><div _ngcontent-otr-c106="" class="highlight-scroll-div"><pre class="cpp prettyprint linenums hljs language-cpp" hw-language="cpp" data-highlighted="yes"><ol class="linenums"><li><span class="hljs-function">OH_NN_ReturnCode <span class="hljs-title">CreateCompilation</span><span class="hljs-params">(OH_NNModel* model, <span class="hljs-type">const</span> std::vector&lt;<span class="hljs-type">size_t</span>&gt;&amp; availableDevice,</span></span></li><li><span class="hljs-function">                                   OH_NNCompilation** pCompilation)</span></li><li><span class="hljs-function"></span>{</li><li>    <span class="hljs-comment">// 创建编译实例compilation，将构图的模型实例或MSLite传下来的模型实例传入</span></li><li>    OH_NNCompilation* compilation = <span class="hljs-built_in">OH_NNCompilation_Construct</span>(model);</li><li>    <span class="hljs-built_in">CHECKEQ</span>(compilation, <span class="hljs-literal">nullptr</span>, OH_NN_FAILED, <span class="hljs-string">"OH_NNCore_ConstructCompilationWithNNModel failed."</span>);</li><li>
</li><li>    <span class="hljs-comment">// 设置编译的硬件、缓存路径、性能模式、计算优先级、是否开启float16低精度计算等选项</span></li><li>    <span class="hljs-comment">// 选择在第一个设备上编译模型</span></li><li>    <span class="hljs-keyword">auto</span> returnCode = <span class="hljs-built_in">OH_NNCompilation_SetDevice</span>(compilation, availableDevice[<span class="hljs-number">0</span>]);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"OH_NNCompilation_SetDevice failed."</span>);</li><li>
</li><li>    <span class="hljs-comment">// 将模型编译结果缓存在/data/local/tmp目录下，版本号指定为1</span></li><li>    returnCode = <span class="hljs-built_in">OH_NNCompilation_SetCache</span>(compilation, <span class="hljs-string">"/data/local/tmp"</span>, <span class="hljs-number">1</span>);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"OH_NNCompilation_SetCache failed."</span>);</li><li>
</li><li>    <span class="hljs-comment">// 设置硬件性能模式</span></li><li>    returnCode = <span class="hljs-built_in">OH_NNCompilation_SetPerformanceMode</span>(compilation, OH_NN_PERFORMANCE_EXTREME);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"OH_NNCompilation_SetPerformanceMode failed."</span>);</li><li>
</li><li>    <span class="hljs-comment">// 设置推理执行优先级</span></li><li>    returnCode = <span class="hljs-built_in">OH_NNCompilation_SetPriority</span>(compilation, OH_NN_PRIORITY_HIGH);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"OH_NNCompilation_SetPriority failed."</span>);</li><li>
</li><li>    <span class="hljs-comment">// 是否开启FP16计算模式</span></li><li>    returnCode = <span class="hljs-built_in">OH_NNCompilation_EnableFloat16</span>(compilation, <span class="hljs-literal">false</span>);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"OH_NNCompilation_EnableFloat16 failed."</span>);</li><li>
</li><li>    <span class="hljs-comment">// 执行模型编译</span></li><li>    returnCode = <span class="hljs-built_in">OH_NNCompilation_Build</span>(compilation);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"OH_NNCompilation_Build failed."</span>);</li><li>
</li><li>    *pCompilation = compilation;</li><li>    <span class="hljs-keyword">return</span> OH_NN_SUCCESS;</li><li>}</li></ol></pre></div></div></li>      <li>       <p>创建执行器。</p>       <p>完成模型编译后，需要调用Neural Network Runtime的执行模块，通过编译实例创建执行器。模型推理阶段中的设置模型输入、触发推理计算以及获取模型输出等操作均需要围绕执行器完成。</p>       <div _ngcontent-otr-c106="" class="highlight-div"><div _ngcontent-otr-c106="" class="highlight-div-header"><div _ngcontent-otr-c106="" class="highlight-div-header-left"><div _ngcontent-otr-c106="" class="handle-button expand-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">收起</div></div></div><div _ngcontent-otr-c106="" class="highlight-div-header-right"><div _ngcontent-otr-c106="" class="handle-button ai-button"></div><div _ngcontent-otr-c106="" class="handle-button line-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">自动换行</div></div><div _ngcontent-otr-c106="" class="handle-button theme-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">深色代码主题</div></div><div _ngcontent-otr-c106="" class="handle-button copy-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">复制</div></div></div></div><div _ngcontent-otr-c106="" class="highlight-scroll-div"><pre class="cpp prettyprint linenums hljs language-cpp" hw-language="cpp" data-highlighted="yes"><ol class="linenums"><li><span class="hljs-function">OH_NNExecutor* <span class="hljs-title">CreateExecutor</span><span class="hljs-params">(OH_NNCompilation* compilation)</span></span></li><li><span class="hljs-function"></span>{</li><li>    <span class="hljs-comment">// 通过编译实例compilation创建执行器executor</span></li><li>    OH_NNExecutor *executor = <span class="hljs-built_in">OH_NNExecutor_Construct</span>(compilation);</li><li>    <span class="hljs-built_in">CHECKEQ</span>(executor, <span class="hljs-literal">nullptr</span>, <span class="hljs-literal">nullptr</span>, <span class="hljs-string">"OH_NNExecutor_Construct failed."</span>);</li><li>    <span class="hljs-keyword">return</span> executor;</li><li>}</li></ol></pre></div></div></li>      <li>       <p>执行推理计算，并打印推理结果。</p>       <p>通过执行模块提供的接口，将推理计算所需要的输入数据传递给执行器，触发执行器完成一次推理计算，获取模型的推理结果并打印。</p>       <div _ngcontent-otr-c106="" class="highlight-div"><div _ngcontent-otr-c106="" class="highlight-div-header"><div _ngcontent-otr-c106="" class="highlight-div-header-left"><div _ngcontent-otr-c106="" class="handle-button expand-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">收起</div></div></div><div _ngcontent-otr-c106="" class="highlight-div-header-right"><div _ngcontent-otr-c106="" class="handle-button ai-button"></div><div _ngcontent-otr-c106="" class="handle-button line-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">自动换行</div></div><div _ngcontent-otr-c106="" class="handle-button theme-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">深色代码主题</div></div><div _ngcontent-otr-c106="" class="handle-button copy-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">复制</div></div></div></div><div _ngcontent-otr-c106="" class="highlight-scroll-div"><pre class="cpp prettyprint linenums hljs language-cpp" hw-language="cpp" data-highlighted="yes"><ol class="linenums"><li><span class="hljs-function">OH_NN_ReturnCode <span class="hljs-title">Run</span><span class="hljs-params">(OH_NNExecutor* executor, <span class="hljs-type">const</span> std::vector&lt;<span class="hljs-type">size_t</span>&gt;&amp; availableDevice)</span></span></li><li><span class="hljs-function"></span>{</li><li>    <span class="hljs-comment">// 从executor获取输入输出信息</span></li><li>    <span class="hljs-comment">// 获取输入张量的个数</span></li><li>    <span class="hljs-type">size_t</span> inputCount = <span class="hljs-number">0</span>;</li><li>    <span class="hljs-keyword">auto</span> returnCode = <span class="hljs-built_in">OH_NNExecutor_GetInputCount</span>(executor, &amp;inputCount);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"OH_NNExecutor_GetInputCount failed."</span>);</li><li>    std::vector&lt;NN_TensorDesc*&gt; inputTensorDescs;</li><li>    NN_TensorDesc* tensorDescTmp = <span class="hljs-literal">nullptr</span>;</li><li>    <span class="hljs-keyword">for</span> (<span class="hljs-type">size_t</span> i = <span class="hljs-number">0</span>; i &lt; inputCount; ++i) {</li><li>        <span class="hljs-comment">// 创建输入张量的描述</span></li><li>        tensorDescTmp = <span class="hljs-built_in">OH_NNExecutor_CreateInputTensorDesc</span>(executor, i);</li><li>        <span class="hljs-built_in">CHECKEQ</span>(tensorDescTmp, <span class="hljs-literal">nullptr</span>, OH_NN_FAILED, <span class="hljs-string">"OH_NNExecutor_CreateInputTensorDesc failed."</span>);</li><li>        inputTensorDescs.<span class="hljs-built_in">emplace_back</span>(tensorDescTmp);</li><li>    }</li><li>    <span class="hljs-comment">// 获取输出张量的个数</span></li><li>    <span class="hljs-type">size_t</span> outputCount = <span class="hljs-number">0</span>;</li><li>    returnCode = <span class="hljs-built_in">OH_NNExecutor_GetOutputCount</span>(executor, &amp;outputCount);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"OH_NNExecutor_GetOutputCount failed."</span>);</li><li>    std::vector&lt;NN_TensorDesc*&gt; outputTensorDescs;</li><li>    <span class="hljs-keyword">for</span> (<span class="hljs-type">size_t</span> i = <span class="hljs-number">0</span>; i &lt; outputCount; ++i) {</li><li>        <span class="hljs-comment">// 创建输出张量的描述</span></li><li>        tensorDescTmp = <span class="hljs-built_in">OH_NNExecutor_CreateOutputTensorDesc</span>(executor, i);</li><li>        <span class="hljs-built_in">CHECKEQ</span>(tensorDescTmp, <span class="hljs-literal">nullptr</span>, OH_NN_FAILED, <span class="hljs-string">"OH_NNExecutor_CreateOutputTensorDesc failed."</span>);</li><li>        outputTensorDescs.<span class="hljs-built_in">emplace_back</span>(tensorDescTmp);</li><li>    }</li><li>
</li><li>    <span class="hljs-comment">// 创建输入和输出张量</span></li><li>    NN_Tensor* inputTensors[inputCount];</li><li>    NN_Tensor* tensor = <span class="hljs-literal">nullptr</span>;</li><li>    <span class="hljs-keyword">for</span> (<span class="hljs-type">size_t</span> i = <span class="hljs-number">0</span>; i &lt; inputCount; ++i) {</li><li>        tensor = <span class="hljs-literal">nullptr</span>;</li><li>        tensor = <span class="hljs-built_in">OH_NNTensor_Create</span>(availableDevice[<span class="hljs-number">0</span>], inputTensorDescs[i]);</li><li>        <span class="hljs-built_in">CHECKEQ</span>(tensor, <span class="hljs-literal">nullptr</span>, OH_NN_FAILED, <span class="hljs-string">"OH_NNTensor_Create failed."</span>);</li><li>        inputTensors[i] = tensor;</li><li>    }</li><li>    NN_Tensor* outputTensors[outputCount];</li><li>    <span class="hljs-keyword">for</span> (<span class="hljs-type">size_t</span> i = <span class="hljs-number">0</span>; i &lt; outputCount; ++i) {</li><li>        tensor = <span class="hljs-literal">nullptr</span>;</li><li>        tensor = <span class="hljs-built_in">OH_NNTensor_Create</span>(availableDevice[<span class="hljs-number">0</span>], outputTensorDescs[i]);</li><li>        <span class="hljs-built_in">CHECKEQ</span>(tensor, <span class="hljs-literal">nullptr</span>, OH_NN_FAILED, <span class="hljs-string">"OH_NNTensor_Create failed."</span>);</li><li>        outputTensors[i] = tensor;</li><li>    }</li><li>
</li><li>    <span class="hljs-comment">// 设置输入张量的数据</span></li><li>    returnCode = <span class="hljs-built_in">SetInputData</span>(inputTensors, inputCount);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"SetInputData failed."</span>);</li><li>
</li><li>    <span class="hljs-comment">// 执行推理</span></li><li>    returnCode = <span class="hljs-built_in">OH_NNExecutor_RunSync</span>(executor, inputTensors, inputCount, outputTensors, outputCount);</li><li>    <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"OH_NNExecutor_RunSync failed."</span>);</li><li>
</li><li>    <span class="hljs-comment">// 打印输出张量的数据</span></li><li>    <span class="hljs-built_in">Print</span>(outputTensors, outputCount);</li><li>
</li><li>    <span class="hljs-comment">// 清理输入和输出张量以及张量描述</span></li><li>    <span class="hljs-keyword">for</span> (<span class="hljs-type">size_t</span> i = <span class="hljs-number">0</span>; i &lt; inputCount; ++i) {</li><li>        returnCode = <span class="hljs-built_in">OH_NNTensor_Destroy</span>(&amp;inputTensors[i]);</li><li>        <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"OH_NNTensor_Destroy failed."</span>);</li><li>        returnCode = <span class="hljs-built_in">OH_NNTensorDesc_Destroy</span>(&amp;inputTensorDescs[i]);</li><li>        <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"OH_NNTensorDesc_Destroy failed."</span>);</li><li>    }</li><li>    <span class="hljs-keyword">for</span> (<span class="hljs-type">size_t</span> i = <span class="hljs-number">0</span>; i &lt; outputCount; ++i) {</li><li>        returnCode = <span class="hljs-built_in">OH_NNTensor_Destroy</span>(&amp;outputTensors[i]);</li><li>        <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"OH_NNTensor_Destroy failed."</span>);</li><li>        returnCode = <span class="hljs-built_in">OH_NNTensorDesc_Destroy</span>(&amp;outputTensorDescs[i]);</li><li>        <span class="hljs-built_in">CHECKNEQ</span>(returnCode, OH_NN_SUCCESS, OH_NN_FAILED, <span class="hljs-string">"OH_NNTensorDesc_Destroy failed."</span>);</li><li>    }</li><li>
</li><li>    <span class="hljs-keyword">return</span> OH_NN_SUCCESS;</li><li>}</li></ol></pre></div></div></li>      <li>       <p>构建端到端模型构造-编译-执行流程。</p>       <p>步骤4-步骤8实现了模型的模型构造、编译和执行流程，并封装成多个函数，便于模块化开发。以下示例代码将串联这些函数， 形成一个完整的Neural Network Runtime使用流程。</p>       <div _ngcontent-otr-c106="" class="highlight-div"><div _ngcontent-otr-c106="" class="highlight-div-header"><div _ngcontent-otr-c106="" class="highlight-div-header-left"><div _ngcontent-otr-c106="" class="handle-button expand-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">收起</div></div></div><div _ngcontent-otr-c106="" class="highlight-div-header-right"><div _ngcontent-otr-c106="" class="handle-button ai-button"></div><div _ngcontent-otr-c106="" class="handle-button line-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">自动换行</div></div><div _ngcontent-otr-c106="" class="handle-button theme-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">深色代码主题</div></div><div _ngcontent-otr-c106="" class="handle-button copy-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">复制</div></div></div></div><div _ngcontent-otr-c106="" class="highlight-scroll-div"><pre class="cpp prettyprint linenums hljs language-cpp" hw-language="cpp" data-highlighted="yes"><ol class="linenums"><li><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-type">int</span> argc, <span class="hljs-type">char</span>** argv)</span></span></li><li><span class="hljs-function"></span>{</li><li>    OH_NNModel* model = <span class="hljs-literal">nullptr</span>;</li><li>    OH_NNCompilation* compilation = <span class="hljs-literal">nullptr</span>;</li><li>    OH_NNExecutor* executor = <span class="hljs-literal">nullptr</span>;</li><li>    std::vector&lt;<span class="hljs-type">size_t</span>&gt; availableDevices;</li><li>
</li><li>    <span class="hljs-comment">// 模型构造</span></li><li>    OH_NN_ReturnCode ret = <span class="hljs-built_in">BuildModel</span>(&amp;model);</li><li>    <span class="hljs-keyword">if</span> (ret != OH_NN_SUCCESS) {</li><li>        std::cout &lt;&lt; <span class="hljs-string">"BuildModel failed."</span> &lt;&lt; std::endl;</li><li>        <span class="hljs-built_in">OH_NNModel_Destroy</span>(&amp;model);</li><li>        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;</li><li>    }</li><li>
</li><li>    <span class="hljs-comment">// 获取可执行的设备</span></li><li>    <span class="hljs-built_in">GetAvailableDevices</span>(availableDevices);</li><li>    <span class="hljs-keyword">if</span> (availableDevices.<span class="hljs-built_in">empty</span>()) {</li><li>        std::cout &lt;&lt; <span class="hljs-string">"No available device."</span> &lt;&lt; std::endl;</li><li>        <span class="hljs-built_in">OH_NNModel_Destroy</span>(&amp;model);</li><li>        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;</li><li>    }</li><li>
</li><li>    <span class="hljs-comment">// 模型编译</span></li><li>    ret = <span class="hljs-built_in">CreateCompilation</span>(model, availableDevices, &amp;compilation);</li><li>    <span class="hljs-keyword">if</span> (ret != OH_NN_SUCCESS) {</li><li>        std::cout &lt;&lt; <span class="hljs-string">"CreateCompilation failed."</span> &lt;&lt; std::endl;</li><li>        <span class="hljs-built_in">OH_NNModel_Destroy</span>(&amp;model);</li><li>        <span class="hljs-built_in">OH_NNCompilation_Destroy</span>(&amp;compilation);</li><li>        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;</li><li>    }</li><li>
</li><li>    <span class="hljs-comment">// 销毁模型实例</span></li><li>    <span class="hljs-built_in">OH_NNModel_Destroy</span>(&amp;model);</li><li>
</li><li>    <span class="hljs-comment">// 创建模型的推理执行器</span></li><li>    executor = <span class="hljs-built_in">CreateExecutor</span>(compilation);</li><li>    <span class="hljs-keyword">if</span> (executor == <span class="hljs-literal">nullptr</span>) {</li><li>        std::cout &lt;&lt; <span class="hljs-string">"CreateExecutor failed, no executor is created."</span> &lt;&lt; std::endl;</li><li>        <span class="hljs-built_in">OH_NNCompilation_Destroy</span>(&amp;compilation);</li><li>        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;</li><li>    }</li><li>
</li><li>    <span class="hljs-comment">// 销毁编译实例</span></li><li>    <span class="hljs-built_in">OH_NNCompilation_Destroy</span>(&amp;compilation);</li><li>
</li><li>    <span class="hljs-comment">// 使用上一步创建的执行器，执行推理计算</span></li><li>    ret = <span class="hljs-built_in">Run</span>(executor, availableDevices);</li><li>    <span class="hljs-keyword">if</span> (ret != OH_NN_SUCCESS) {</li><li>        std::cout &lt;&lt; <span class="hljs-string">"Run failed."</span> &lt;&lt; std::endl;</li><li>        <span class="hljs-built_in">OH_NNExecutor_Destroy</span>(&amp;executor);</li><li>        <span class="hljs-keyword">return</span> <span class="hljs-number">-1</span>;</li><li>    }</li><li>
</li><li>    <span class="hljs-comment">// 销毁执行器实例</span></li><li>    <span class="hljs-built_in">OH_NNExecutor_Destroy</span>(&amp;executor);</li><li>
</li><li>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;</li><li>}</li></ol></pre></div></div></li>     </ol>    </div>    <div class="tiledSection">     <h2 id="调测验证">调测验证<i class="anchor-icon anchor-icon-link" anchorid="调测验证" tips="复制节点链接"></i></h2>          <ol>      <li>       <p>准备应用样例的编译配置文件。</p>       <p>新建一个 CMakeLists.txt 文件，为开发步骤中的应用样例文件 nnrt_example.cpp 添加编译配置。以下提供简单的 CMakeLists.txt 示例：</p>       <div _ngcontent-otr-c106="" class="highlight-div"><div _ngcontent-otr-c106="" class="highlight-div-header"><div _ngcontent-otr-c106="" class="highlight-div-header-left"><div _ngcontent-otr-c106="" class="handle-button expand-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">收起</div></div></div><div _ngcontent-otr-c106="" class="highlight-div-header-right"><div _ngcontent-otr-c106="" class="handle-button ai-button"></div><div _ngcontent-otr-c106="" class="handle-button line-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">自动换行</div></div><div _ngcontent-otr-c106="" class="handle-button theme-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">深色代码主题</div></div><div _ngcontent-otr-c106="" class="handle-button copy-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">复制</div></div></div></div><div _ngcontent-otr-c106="" class="highlight-scroll-div"><pre class="text prettyprint linenums hljs language-plaintext" hw-language="text" data-highlighted="yes"><ol class="linenums"><li>cmake_minimum_required(VERSION 3.16)</li><li>project(nnrt_example C CXX)</li><li>
</li><li>add_executable(nnrt_example</li><li>    ./nnrt_example.cpp</li><li>)</li><li>
</li><li>target_link_libraries(nnrt_example</li><li>    neural_network_runtime</li><li>    neural_network_core</li><li>)</li></ol></pre></div></div></li>      <li>       <p>编译应用样例。</p>       <p>执行以下命令，在当前目录下新建build/目录，在build/目录下编译 nnrt_example.cpp，得到二进制文件 nnrt_example。</p>       <div _ngcontent-otr-c106="" class="highlight-div"><div _ngcontent-otr-c106="" class="highlight-div-header"><div _ngcontent-otr-c106="" class="highlight-div-header-left"><div _ngcontent-otr-c106="" class="handle-button expand-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">收起</div></div></div><div _ngcontent-otr-c106="" class="highlight-div-header-right"><div _ngcontent-otr-c106="" class="handle-button ai-button"></div><div _ngcontent-otr-c106="" class="handle-button line-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">自动换行</div></div><div _ngcontent-otr-c106="" class="handle-button theme-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">深色代码主题</div></div><div _ngcontent-otr-c106="" class="handle-button copy-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">复制</div></div></div></div><div _ngcontent-otr-c106="" class="highlight-scroll-div"><pre class="shell prettyprint linenums hljs language-shell" hw-language="shell" data-highlighted="yes"><ol class="linenums"><li>mkdir build &amp;&amp; cd build</li><li>cmake -DCMAKE_TOOLCHAIN_FILE={交叉编译工具链的路径}/build/cmake/ohos.toolchain.cmake -DOHOS_ARCH=arm64-v8a -DOHOS_PLATFORM=OHOS -DOHOS_STL=c++_static ..</li><li>make</li></ol></pre></div></div></li>      <li>       <p>执行以下代码，将样例推送到设备上执行。</p>       <div _ngcontent-otr-c106="" class="highlight-div"><div _ngcontent-otr-c106="" class="highlight-div-header"><div _ngcontent-otr-c106="" class="highlight-div-header-left"><div _ngcontent-otr-c106="" class="handle-button expand-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">收起</div></div></div><div _ngcontent-otr-c106="" class="highlight-div-header-right"><div _ngcontent-otr-c106="" class="handle-button ai-button"></div><div _ngcontent-otr-c106="" class="handle-button line-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">自动换行</div></div><div _ngcontent-otr-c106="" class="handle-button theme-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">深色代码主题</div></div><div _ngcontent-otr-c106="" class="handle-button copy-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">复制</div></div></div></div><div _ngcontent-otr-c106="" class="highlight-scroll-div"><pre class="shell prettyprint linenums hljs language-shell" hw-language="shell" data-highlighted="yes"><ol class="linenums"><li><span class="hljs-meta prompt_"># </span><span class="language-bash">将编译得到的 `nnrt_example` 推送到设备上，执行样例。</span></li><li>hdc_std file send ./nnrt_example /data/local/tmp/.</li><li><span class="hljs-meta prompt_"></span></li><li><span class="hljs-meta prompt_"># </span><span class="language-bash">给测试用例可执行文件加上权限。</span></li><li>hdc_std shell "chmod +x /data/local/tmp/nnrt_example"</li><li><span class="hljs-meta prompt_"></span></li><li><span class="hljs-meta prompt_"># </span><span class="language-bash">执行测试用例</span></li><li>hdc_std shell "/data/local/tmp/nnrt_example"</li></ol></pre></div></div>       <p>如果样例执行正常，应该得到以下输出。</p>       <div _ngcontent-otr-c106="" class="highlight-div"><div _ngcontent-otr-c106="" class="highlight-div-header"><div _ngcontent-otr-c106="" class="highlight-div-header-left"><div _ngcontent-otr-c106="" class="handle-button expand-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">收起</div></div></div><div _ngcontent-otr-c106="" class="highlight-div-header-right"><div _ngcontent-otr-c106="" class="handle-button ai-button"></div><div _ngcontent-otr-c106="" class="handle-button line-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">自动换行</div></div><div _ngcontent-otr-c106="" class="handle-button theme-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">深色代码主题</div></div><div _ngcontent-otr-c106="" class="handle-button copy-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">复制</div></div></div></div><div _ngcontent-otr-c106="" class="highlight-scroll-div"><pre class="text prettyprint linenums hljs language-plaintext" hw-language="text" data-highlighted="yes"><ol class="linenums"><li>Output index: 0, value is: 0.000000.</li><li>Output index: 1, value is: 2.000000.</li><li>Output index: 2, value is: 4.000000.</li><li>Output index: 3, value is: 6.000000.</li><li>Output index: 4, value is: 8.000000.</li><li>Output index: 5, value is: 10.000000.</li><li>Output index: 6, value is: 12.000000.</li><li>Output index: 7, value is: 14.000000.</li><li>Output index: 8, value is: 16.000000.</li><li>Output index: 9, value is: 18.000000.</li><li>Output index: 10, value is: 20.000000.</li><li>Output index: 11, value is: 22.000000.</li></ol></pre></div></div></li>      <li>       <p>检查模型缓存（可选）。</p>       <p>如果在调测环境下，Neural Network Runtime对接的HDI服务支持模型缓存功能，执行完 nnrt_example, 可以在 /data/local/tmp 目录下找到生成的缓存文件。</p>       <div><div class="hw-editor-tip info"><div class="title">说明</div><div class="content">         <p>模型的IR需要传递到硬件驱动层，由HDI服务将统一的IR图，编译成硬件专用的计算图，编译的过程非常耗时。Neural Network Runtime支持计算图缓存的特性，可以将HDI服务编译生成的计算图，缓存到设备存储中。当下一次在同一个加速芯片上编译同一个模型时，通过指定缓存的路径，Neural Network Runtime可以直接加载缓存文件中的计算图，减少编译消耗的时间。</p>        </div></div></div>       <p>检查缓存目录下的缓存文件：</p>       <div _ngcontent-otr-c106="" class="highlight-div"><div _ngcontent-otr-c106="" class="highlight-div-header"><div _ngcontent-otr-c106="" class="highlight-div-header-left"><div _ngcontent-otr-c106="" class="handle-button expand-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">收起</div></div></div><div _ngcontent-otr-c106="" class="highlight-div-header-right"><div _ngcontent-otr-c106="" class="handle-button ai-button"></div><div _ngcontent-otr-c106="" class="handle-button line-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">自动换行</div></div><div _ngcontent-otr-c106="" class="handle-button theme-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">深色代码主题</div></div><div _ngcontent-otr-c106="" class="handle-button copy-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">复制</div></div></div></div><div _ngcontent-otr-c106="" class="highlight-scroll-div"><pre class="shell prettyprint linenums hljs language-shell" hw-language="shell" data-highlighted="yes"><ol class="linenums"><li>ls /data/local/tmp</li></ol></pre></div></div>       <p>以下为打印结果：</p>       <div _ngcontent-otr-c106="" class="highlight-div"><div _ngcontent-otr-c106="" class="highlight-div-header"><div _ngcontent-otr-c106="" class="highlight-div-header-left"><div _ngcontent-otr-c106="" class="handle-button expand-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">收起</div></div></div><div _ngcontent-otr-c106="" class="highlight-div-header-right"><div _ngcontent-otr-c106="" class="handle-button ai-button"></div><div _ngcontent-otr-c106="" class="handle-button line-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">自动换行</div></div><div _ngcontent-otr-c106="" class="handle-button theme-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">深色代码主题</div></div><div _ngcontent-otr-c106="" class="handle-button copy-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">复制</div></div></div></div><div _ngcontent-otr-c106="" class="highlight-scroll-div"><pre class="text prettyprint linenums hljs language-plaintext" hw-language="text" data-highlighted="yes"><ol class="linenums"><li># 0.nncache 1.nncache 2.nncache cache_info.nncache</li></ol></pre></div></div>       <p>如果缓存不再使用，需要手动删除缓存，可以参考以下命令，删除缓存文件。</p>       <div _ngcontent-otr-c106="" class="highlight-div"><div _ngcontent-otr-c106="" class="highlight-div-header"><div _ngcontent-otr-c106="" class="highlight-div-header-left"><div _ngcontent-otr-c106="" class="handle-button expand-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">收起</div></div></div><div _ngcontent-otr-c106="" class="highlight-div-header-right"><div _ngcontent-otr-c106="" class="handle-button ai-button"></div><div _ngcontent-otr-c106="" class="handle-button line-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">自动换行</div></div><div _ngcontent-otr-c106="" class="handle-button theme-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">深色代码主题</div></div><div _ngcontent-otr-c106="" class="handle-button copy-button"><div _ngcontent-otr-c106="" class="handle-hover-tips">复制</div></div></div></div><div _ngcontent-otr-c106="" class="highlight-scroll-div"><pre class="shell prettyprint linenums hljs language-shell" hw-language="shell" data-highlighted="yes"><ol class="linenums"><li>rm /data/local/tmp/*nncache</li></ol></pre></div></div></li>     </ol>    </div>   </div>   <div></div></div>